---
title: "Bayesian analysis of Organic Orthogonal Phase Seperation Data"
author: "Oliver M. Crook"
date: "22/09/2021"
output:
    BiocStyle::html_document:
        toc_float: true
geometry: margin = 2cm        
---

```{r style, echo = FALSE, results = 'asis', message=FALSE}
BiocStyle::markdown()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(dpi=300,fig.width=10)
```

# Introduction

Here, we attempt to build a principled Bayesian model for some OOPS data.
We illustrate the main ideas of a Bayesian workflow and refer to the 
accompanying manuscript for relevent details. The first code chunk loads the 
packages we are interested in. 

```{r,}
library(brms)
library(pheatmap)
library(RColorBrewer)
library(MSnbase)
library(ggplot2)
library(ggfortify)
bayesplot::color_scheme_set("teal")
```


Now, we load the data we are interested in, which is from Queiroz et al. 2019. 
The data have already been summarised to protein level and column median normalised.
The design used 2 seperate TMT 10-plexes to measure total protein abundance and 
oops samples. This allows us to model changes in RNA binding that are independent
of concurrent changes in total protien abundance. Changes in oops-enriched
protein abundance samples that do not correlated with total protein
abundance changes are indicative of specific RBPs binding RNA differentially
in those conditions. The experimental design uses thymidine-nocodazole arrest
and then measures protein abundance, as well as oops enriched protein
abundance at $0,6$ and $23$ hours. Triplicate measurements are made, except for
at 6 hours where 4 replicates where taken. 
For further details see Queiroz et al. 2019
```{r,}
# We assume the data is in the working directory
oopsdata <- readMSnSet2(file = "oopsdata.csv", ecol = 2:21, fnames = 1)
```

First, it is a good idea to get a feel for the data using some exploratory data
analysis. Below we produce a heatmap. We can easily see from this data that there
are some differences between the total and oops samples, as well as differences
between the timepoints.
```{r, fig.height = 8, fig.width = 12, fig.align = "center"}
pheatmap(exprs(oopsdata), col = brewer.pal(n = 10, name = "PRGn"), cluster_cols = FALSE)

```
Now we perform PCA to see if there is any systematic clustering. There
are clear differences between the oops and total samples, as well
as a difference between 6h oops and 23h oops samples. 
```{r,}
autoplot(prcomp(t(exprs(oopsdata))), label = TRUE) + theme_classic()
```
Now, we look at a global boxplot across the columns. There looks to fairly
consistent distributions of intensities across all the columns and perhaps
slightly higher variance for the oops samples. We visualise
the varainces in a boxplot
```{r,}
boxplot(exprs(oopsdata), col = rep(brewer.pal(n = 2, name = "Set2"), each = 10))
barplot(apply(exprs(oopsdata),2,var), col = rep(brewer.pal(n = 2, name = "Set2"), each = 10))
```

# Initial modelling thoughts

The first step is to establish what questions and aspects of the experimental 
design we are interested in modelling. Ultimately, we are interested in looking
at RBPs binding RNA differentially. However, simply a change in abundance
will make it appear like there is more RNA binding. We need to control 
simultaniously for the changes in abundance. There are also changes across
the different times points. We also make replicate measurements and should
allow some modelled variation for replicates. There are also TMT tags, we might
expect some variation due to which TMT tag was used. The variance is also
higher for the oops sample so it may be sensible to allow the variance to depend
on the sample type. 

We first buid the simplest model that answers the question we are interested in.
We account for time and sample type (oops vs total) and the interaction between them.
The interaction allows us to examine RBPs that bind RNA differentially. The model
looks like following for each protein, with Gaussian noice 
$$
\log abundance = total + time + total*time
$$
We test using a single protein for simplicity, the code chunk below extracts 
this protein.
```{r,}
set.seed(1)
data <- data.frame(exprs(oopsdata)[sample.int(nrow(oopsdata), size = 1),])
colnames(data) <- "abundance"
data$type <- as.factor(rep(c("total", "oops"), each = 10))
data$time <- as.factor(rep(rep(c("0", "6", "23"), times = c(3,4,3)), 2))
data$replicate <- as.factor(sapply(strsplit(rownames(data), "_"), function(x)x[2]))
data$tag <- as.factor(rep(TMT10@reporterNames, times = 2))
```

# Fitting our first model
Let us start the workflow by setting some priors and the model. In the code
chunk below, we pick some roughly arbitrary priors to get started. When 
developing your own model it can be useful to use the `get_prior` function to 
know which priors to set. 

Below, we have asked the code just to return samples from the prior so we can see
what effect they have in practice. We can then visualise some summaries of the
data.


```{r,}
pr <- c(prior(normal(0,1), class = "b"),
  prior(exponential(1), class = "sigma"))


fit_prot1 <- brm(abundance ~ 0 + type + time + (type:time),
                 data = data,
                 family = gaussian(link = "identity"),
                 sample_prior = "only",
                 prior = pr)
plot(fit_prot1, ask = FALSE)
summary(fit_prot1)
```
We make a few initial observation. For the moment is appears that the parameters
are on a sensible scale. There are a few aspects we should be mindful of as we
progress through the modelling. The first is the values for $\sigma$, which can
occasionally get unrealistically large. For these data standard deviations above
$2.5$ should be extremely rare, whist we see from the prior simulations that this
can happen not infrequently. We also note, from examining the PCA plot, that
the effect size for type (oops or total) is likely to be larger than that for 
time. Furthermore, it seems unlikely that we expect interaction effects to be
on the same scale as type changes. One might expect that after account for
the type and time that the interaction would have a smaller effect.

# Prior Predictive check
The next step in the modelling is a prior predictive check. Here, we produce
some summary of the data and summaries of the data simulated from the prior
predictive distribution. A huge number of different checks are available.
Visually the simulated samples ought to "look like" the observed data.
```{r,}
# compute kernel density estimate of data and simulated data and plot
pp_check(object = fit_prot1, "dens_overlay") 
# plot data in a histogram to see distributions
pp_check(object = fit_prot1, "hist")
# plot data in a boxplot to see distribtuions
pp_check(object = fit_prot1, "boxplot")
# produce summary statistics
pp_check(object = fit_prot1, "stat_2d")
# produce intervals
pp_check(object = fit_prot1, "intervals")
# predctive errors
pp_check(object = fit_prot1, "error_hist")
```
These prior predictive checks suggest that the prior is centered correctly,
but is very diffuse. In principle, a diffuse prior might not be a problem if
the data are sufficiently informative. However, for proteomics data we are unlikely
to make more than 3 or 4 measurements and it maybe sensible to build a better 
generative model before we move further into the modelling task.

Let's us be a little more mathematical than our previous approach. The standard
deviations are unlikely to exceed $2.5$ in this proteomics experiments. However,
an `exponential(1)` prior leads to standard deviations above 2.5 around 9% of the
time (see code chunk below). For us rare would be 1 in 5000 proteins so this 
is clearly miscalibrated with our expectations. We can see that an exponential 
with rate of $4$ capture this expectation. 

```{r,}
# probability of observing sd above 2.5
sum(rexp(1000, rate = 1) > 2.5)/1000 
# with rate 4, probability of observing a sd above 2.5 is 1/5000
sum(rexp(10000, rate = 4) > 2.5)/10000 
```

However, as we push the prior through the model is not clear how changing
the prior on the standard deviation will be reflected in generated data. We
can repeat our prior predictive checks. Below we fit the model without data
as before. Note that values for $\sigma$ are now more aligned with our expectations.

```{r,}
pr <- c(prior(normal(0,1), class = "b"),
  prior(exponential(4), class = "sigma"))


fit_prot1 <- brm(abundance ~ 0 + type + time + (type:time),
                 data = data,
                 family = gaussian(link = "identity"),
                 sample_prior = "only",
                 prior = pr)
plot(fit_prot1, ask = FALSE)
summary(fit_prot1)
```

From the prior predictive checks below we see that the simulated data are much 
less diffuse in the observed data than before, whilst still being fairly broad
in the range of data we could model.
```{r,}
# compute kernel density estimate of data and simulated data and plot
pp_check(object = fit_prot1, "dens_overlay") 
# plot data in a histogram to see distributions
pp_check(object = fit_prot1, "hist")
# plot data in a boxplot to see distribtuions
pp_check(object = fit_prot1, "boxplot")
# produce summary statistics
pp_check(object = fit_prot1, "stat_2d")
# produce intervals
pp_check(object = fit_prot1, "intervals")
# predctive errors
pp_check(object = fit_prot1, "error_hist")
```

# Fitting the model to the data

Let us now examine the model fitted to the data.
```{r,}
pr <- c(prior(normal(0,1), class = "b"),
  prior(exponential(4), class = "sigma"))


fit_prot1_post<- brm(abundance ~ 0 + type + time + (type:time),
                 data = data,
                 family = gaussian(link = "identity"),
                 sample_prior = "no",
                 prior = pr)
plot(fit_prot1_post, ask = FALSE)
summary(fit_prot1_post)
```
Before, we dive into what the model is telling us we check that our Bayesian
computations have been faithful. Here `Rhat` is about $1$ and we can assume 
that it is unlikely there are any computational obstructions.

The above demonstrates the posterior distributions have clearly learnt something
from the data. Let us have a look at one of the distributions for the parameters.
The prior in orange is clearly spread out compared to the posterior distribution
in green. It looks like the data was very informative of this parameter.
```{r,}
hist(posterior_samples(fit_prot1)[,1],
     main = "Posterior Distribution type=oops",
     breaks = 20, col = brewer.pal(n = 3, name = "Set2")[2], xlab = "estimate")
hist(posterior_samples(fit_prot1_post)[,1],
     main = "Posterior Distribution type=oops",
     breaks = 30, col = brewer.pal(n = 3, name = "Set2")[1], xlab = "estimate", add = TRUE)
```
We can compute the posterior contraction to how much concentration there is of
the posterior. The value is close to 1 suggesting the parameters is well-informed
by the data.
```{r,}
contraction <- 1 - (sd(posterior_samples(fit_prot1_post)[,1])/sd(posterior_samples(fit_prot1)[,1]))
contraction
```
Now let us have a look at the posterior predictive checks. From each of the plots
below we can see that the posterior predictive distribution matches the summaries
of the data extremely well. This suggests that data generated by using the 
posterior estimates of the parameters look like the data we observed.
```{r,}
# compute kernel density estimate of data and simulated data and plot
pp_check(object = fit_prot1_post, "dens_overlay") 
# plot data in a histogram to see distributions
pp_check(object = fit_prot1_post, "hist")
# plot data in a boxplot to see distribtuions
pp_check(object = fit_prot1_post, "boxplot")
# produce summary statistics
pp_check(object = fit_prot1_post, "stat_2d")
# produce intervals
pp_check(object = fit_prot1_post, "intervals")
# predctive errors
pp_check(object = fit_prot1_post, "error_hist")
```
We notice that there is a small bump at the lower end of the first density plot.
We can generate more samples to see if this bump is something we can generate
from the data. From the plot below it shows we can generate similar to that
observed. However, there is a concern that simulated data is perhaps producing
data more extreme that that observed, as well as occasionally some unrealistic
looking data.

```{r,}
pp_check(object = fit_prot1_post, "dens_overlay", nsamples = 50) 
```

# Using uncertainty

Before we investigate more complex models, we examine what the 
uncertainty can do for us to understand our model, inferences and problem.
Recall, we are particularly interested in the direction between type and time.
We first look at the interaction at 6 hours with type. As we can see from
the plot the estimate for the interaction is mostly negative. This suggests
that *relative* to the total sample the oops sample increased. This may 
lead us to believe that the proprotion of the protein that is RNA-bound is
greater.
```{r,}
hist(posterior_samples(fit_prot1_post)[,6], 
     breaks = 30, col = brewer.pal(n = 3, name = "Set2")[1],
     xlab = "estimate", 
     main = "Posterior estimates for interaction between total and 6 h")
```
Since we are working with probability distributions, we can simply calculate
the probability that this estimate is less than 0. We see this is $0.98525$ and
we can also compute other probabilities as well. For example, we may only
be interested in meaningful effect sizes. 
```{r,}
# Probability less than 0
sum(posterior_samples(fit_prot1_post)[,6] < 0)/length(posterior_samples(fit_prot1_post)[,6])
# Probability less than -0.1
sum(posterior_samples(fit_prot1_post)[,6] < -0.1)/length(posterior_samples(fit_prot1_post)[,6])
```
We can also examine the interaction effect at 23h, where we see the effect is 
in the opposite direction. 
```{r,}
hist(posterior_samples(fit_prot1_post)[,5], 
     breaks = 30, col = brewer.pal(n = 3, name = "Set2")[2],
     xlab = "estimate", 
     main = "Posterior estimates for interaction between total and 23 h")
```
We can calculate probabilistic quantities from this distribution and we can
see the effect is much weaker in this case. However, there is still some evidence
for an decrease in the proportion of this protein binding RNA relative to total
protein at 23h post noczodole arrest.
```{r,}
# Probability greater than 0
sum(posterior_samples(fit_prot1_post)[,5] > 0)/length(posterior_samples(fit_prot1_post)[,5])
# Probability greater than -0.1
sum(posterior_samples(fit_prot1_post)[,5] > 0.1)/length(posterior_samples(fit_prot1_post)[,5])
```
We can also ask more elaborate probabilistic questions of these data. We can
ask the probability that the changes for the interaction terms
were in the same direction at 6 hours and 23 hours. We see that the probability
that the effect is in same direction is quite small and more likely the effects
are in different directions. 
```{r,}
probBothPositive <- sum(posterior_samples(fit_prot1_post)[,5] > 0 & 
                            posterior_samples(fit_prot1_post)[,6] > 0)/4000
probBothNegative <- sum(posterior_samples(fit_prot1_post)[,5] < 0 & 
                            posterior_samples(fit_prot1_post)[,6] < 0)/4000
probBothNegative + probBothPositive
```
We can also compute the probability that the effects are greater than $0.1$ at
both times in any direction. We can see we are quite uncertain as to whether
the effects are both simultaneously greater than $0.1$. Of course we can ask
more complex probability questions in the same manner.
```{r,}
sum(abs(posterior_samples(fit_prot1_post)[,5]) > 0.1 & 
        abs(posterior_samples(fit_prot1_post)[,6]) > 0.1)/4000
```

Credible interval can be obtained from the summary.
```{r,}
summary(fit_prot1_post)
```
This provides the basics of the workflow. We have a probabilistic model
of which we can ask questions and quantify our uncertainties.

