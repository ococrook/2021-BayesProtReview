\documentclass[12pt,english, journal=jpr, layout=twocolumn]{article}
\usepackage{amsmath,mathtools,amssymb,mathrsfs,dsfont,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{color}
\graphicspath{ {figures/} }
\usepackage[colorlinks]{hyperref}
\hypersetup{citecolor=blue}
\usepackage{enumitem}
\usepackage{authblk}
\usepackage{pifont}
\usepackage{lineno}
\usepackage[normalem]{ulem}
\usepackage[export]{adjustbox}
\usepackage{ccaption}
\usepackage{achemso}
\linespread{1.1}

\renewcommand\Affilfont{\itshape\scriptsize}
\renewcommand\Authfont{\small}
\newcommand{\xmark}{\ding{55}}
%opening
\title{Challenges and opportunities for Bayesian statistics in proteomics}
\author[1]{Oliver M. Crook \thanks{\url{oliver.crook@stats.ox.ac.uk}}~}
\author[2]{Chun-wa Chung}
\author[1]{Charlotte M. Deane}
	
	
\affil[1]{Department of Statistics, University of Oxford, Oxford, UK}
\affil[2]{Structural and Biophysical Sciences, GlaxoSmithKline R\&D, Stevenage, UK}

\begin{document}
\maketitle
\begin{abstract}
Proteomics is a data-rich science with complex experimental designs and an intricate measurement process. To obtain insights from the large datasets produced, statistical methods, including machine learning are routinely applied. For a quantity of interest, many of these approaches only produce a point estimate, such as a mean, leaving little room for more nuanced interpretations. By contrast, Bayesian statistics allows quantification of uncertainty through the use of probability distributions. These probability distributions enable scientists to ask complex questions of their proteomics data. Bayesian statistics also offers a modular framework for data analysis by making dependencies between data and parameters explicit. Hence, specifying complex hierarchies of parameter dependencies is straightforward in the Bayesian framework. This allows us to use statistical methodology which equals, rather than neglects, the sophistication of experimental design and instrumentation present in proteomics. Here, we review Bayesian methods applied to proteomics, demonstrating its potential power, alongside the challenges posed by adopting this new statistical framework. To illustrate our review, we give a walk-through of the development of a Bayesian model for dynamic organic orthogonal phase-separation (OOPS) data.      
\end{abstract}
\section{Introduction}
Decision making spans the entire research process. Ultimately, it is a choice to believe an explanation for a phenomena given the current evidence. For some theories, the evidence is overwhelming: careful mechanistic experiments and verifiable model predictions have never contradicted that theory \citep{Wilkie::1974}. This scenario is, however, rare. In practice, we make decisions under uncertainty and the evidence is not clear-cut \citep{Tversky::1974}. Bayesian statistics allows us to make inferences from that evidence to enable decision making in those cases \citep{Gelman::1995}. In contrast to \textit{frequentist} methods, Bayesian inference allows us to use probability to model degrees of belief, rather than just frequencies. Consequently, models that are consistent with the available evidence are more probable and incompatible models are less probable \citep{Gelman::2020, Schad::2021}. By using probability theory in this manner, there is a recipe for taking \textit{prior beliefs} (i.e. information encoded by domain expertise) and updating them to \textit{posterior beliefs} using observed data \citep{Gelman::1995}. This posterior probability distribution quantifies the models compatible with domain expertise and our experimental data \citep{Gelman::2020}. This recipe is known more formally as \textit{Bayes' theorem}. 

Mass-spectrometry-based proteomics is a complex scientific field \citep{Bantscheff::2007}. The techniques versatility allows it to explore differential abundance \citep{Bantscheff::2007}, protein turnover \citep{Mathieson::2018}, interactions \citep{Huttlin::2015}, thermal stability \citep{Mateus::2020}, structure \citep{Schopper::2017,Masson::2019}, spatial information \citep{Gessel::2014, Geladaki::2019, Barylyuk::2020} and more \citep{Toby::2016, Nightingale::2018, Johnson::2019}. In each case, data are manipulated, thresholded and filtered so that a statistical test or machine learning algorithm can be applied. The results are then frequently a single value, which is granted the role of arbiter of truth. Bayesian statistics allows the propagation or quantification of uncertainty in all of the step of an analysis, replacing the current implicit ad-hoc approaches with explicit models and summarises the output with a probability distribution consistent with the data \citep{Gelman::2020}. This paradigm progression not only provides an ability to ask new questions of the data but a consistent way to perform inference and criticize models \citep{Gelman::1995, Gelman::2020}. 

Bayesian statistics offers considerable potential for the examination of proteomics data; despite this, it has not been readily adopted in the community. Here, we review the contribution Bayesian statistics has already made to proteomics, clarify the Bayesian workflow and how it can be applied, highlight a number of modelling strategies and outline current challenges for the proteomics community. Throughout, we illustrate our analysis with examples from the proteomics literature, focusing on building a model for dynamic organic orthogonal phase separation (oops) data \citep{Queiroz::2019}. \textcolor{red}{This structure is designed to inform a number of readers including those unfamiliar with Bayesian statistics and those developing tools to analyse proteomic data.}




\section{Main}
\subsection{Bayes, in brief}
Before reviewing the contributions Bayesian statistics has already made to proteomics, we introduce the technical background and notation. We use $P(E)$ to denote the probability of the event $E$. $E$ can be anything from "it rains tomorrow" to "my parameter falls between the values $a$ and $b$". We let $D$ be notation for the observed data, for example from a shotgun proteomics experiment and let $x$ be a data point from $D$, such as a measurement for a particular protein. We assume that $x$ is a sample from some probability distribution $p$ and we write $x \sim p(x|\theta)$, for example this could be a log normal distribution \textcolor{red}{(see Figure \ref{figure::figure1dist})}.\textcolor{red}{ A \textit{sample} is a random draw from that probability distribution, which can be thought of picking out a number from a bag where the probability of selecting that number is defined by the probability distribution. The notation $p(x|\theta)$ describes the probability distribution, where $x$ is the variable. The vertical bar means "given", so that everything on the right-hand side is assumed to be known. For example $\theta$ for a normal distribution is the mean and variance. }


 Let $\alpha$ be hyperparameters of the parameter distribution, such that the $\theta$ themselves are drawn from a probability distribution $\theta \sim p(\theta|\alpha)$. For example, the log-mean parameter of the log normal distribution could be drawn from a normal distribution. \textcolor{red}{The log normal distribution has two parameters the log-mean $m$ and log-variance $\sigma^2$ parameter, which are not equal to the mean and variance of the distribution. In the case of the log normal distribution, we have that $\theta = (m, \sigma^2)$ and note that $m$ is any number, whilst $\sigma$ must be positive. From here, we can define a probability distribution for $m$ by letting $m \sim \mathcal{N}(m|\mu,\nu^2)$ and hence $\alpha = (\mu, \nu^2)$. In figure \ref{figure::figure1dist}, we plot the distribution defined by $\nu^2 = \sigma^2 = 1$ and $\mu = 0$, which we can see has higher mean and variance than the log-normal distribution with log-mean parameter $0$ and log-variance parameter $1$. Such \textit{hierarchical distributions} are useful for describing scenarios where the probabilities depend on levels of information. For example, we could define the probability distribution of it raining today given that it is cloudy: $p(\text{rain}|\text{cloudy today}) = 0.8$ and $p(\text{rain}|\text{not cloudy today}) = 0.1$. This distribution depends on the parameter cloudy or not, which also occurs with some probability i.e. the probability that it is cloudy today given that it was cloudy yesterday could be given by $p(\text{cloudy today}| \text{cloudy yesterday}) = 0.3$). The flexibility of hierarchical distributions allows us to model an array of scenarios.}  

The \textit{prior distribution} captures our domain expertise and is the distribution of the parameters before any data is observed: $p(\theta|\alpha)$. The \textit{prior} could capture, say, that abundance values are positive and are unlikely to exceed the number of grains of sand on Earth. The sampling distribution is the distribution of the data given the parameters $p(D|\theta)$, we can write this as a function $L(\theta|D)$ called the \textit{likelihood}. If we average (or \textit{marginalise}) the distribution of the data over the parameters, we obtain the so-called marginal likelihood:
\begin{equation}
p(D|\alpha) = \int L(\theta|D)p(\theta|\alpha)\,\text{d}\theta.
\end{equation}
\textcolor{red}{We can see that the marginal likelihood is the likelihood multiplied (or weighted) by the prior and then integrated over the parameters. The symbol $\int$ denotes integration, an operation which assigns numbers to functions. For example, the area under a curve is given by the integral of the function describing that curve. Integration arises in probability because we can colloquially think of probability as being the "area" of an event on a distribution. The posterior distribution is the distribution of the parameters after having observed data and is determined by Bayes' theorem, as the following:}
\begin{equation}
p(\theta|D, \alpha) = \frac{p(D|\theta, \alpha)p(\theta|\alpha)}{p(D|\alpha)}.
\end{equation}
Bayes' theorem tells us the mathematical way to update beliefs in light of evidence: simply multiply our prior and likelihood and renormalise by the marginal likelihood. \textcolor{red}{Returning to our weather example, we can demonstrate an application of Bayes' theorem. Given that is rained today and it was cloudy yesterday and it was cloudy yesterday, what was the probability that it was cloudy today? Formally, we are being asked for
\scriptsize
\begin{equation}
\begin{split}
p(&\text{cloudy today}|\text{rain,cloudy yesterday}) \\
& = \left\{\text{Apply Bayes' theorem}\right\}\\
& = \frac{p(\text{rain}|\text{cloudy today, cloudy yesterday})p(\text{cloudy today|\text{cloudy yesterday}})}{p(\text{rain}|\text{cloudy yesterday})}\\
& = \left\{\text{Write out marginal likelihood}\right\}\\
& = \frac{p(\text{rain}|\text{cloudy today, cloudy yesterday})p(\text{cloudy today|\text{cloudy yesterday}})}{p(\text{rain}|\text{cloudy today})p(\text{cloudy today|cloudy yesterday}) + p(\text{rain}|\text{not cloudy today})p(\text{not cloudy today|cloudy yesterday}) }\\
& = \left\{\text{Substitute values}\right\}\\
& = \frac{0.8 \times 0.3}{(0.8 \times 0.3 + 0.1 \times 0.7)} = \frac{24}{31}.
\end{split}	
\end{equation}}
Bayes' theorem implies a self-consistency property: the posterior averaged over the data returns the prior \citep{Talts::2018}:
\begin{equation}
p(\theta) = \int \int p(\theta|\tilde{y})p(\tilde{y}|\tilde{\theta})p(\tilde{\theta})\,\text{d}\tilde{y}\text{d}\tilde{\theta},
\end{equation}
where $\tilde{y} \sim p(D|\tilde{\theta})$. \textcolor{red}{This can also be thought of in terms of simulation. First, sample parameter values from the prior distribution of the parameters. Then sample data from the model given these parameter values. Then given these data sample from the posterior distribution of the parameters. These samples will also be distributed according to the prior distribution.} In Bayesian analysis to perform prediction, instead of simply taking a single parameter value forward, we use averaging, \textcolor{red}{which results in a distribution of values}:
\begin{equation}
p(\tilde{x}|D,\alpha) = \int p(\tilde{x}|\theta)p(\theta|X,\alpha)\,\text{d}\theta.
\end{equation}
In summary, Bayesian statistics provides us with a distribution of plausible parameter values from the \textit{posterior} and a distribution of hypothetical predicted values from the \textit{posterior predictive distribution}. \textcolor{red}{It is these distributions that quantify uncertainty in a Bayesian analysis.} We can then ask bespoke question of these probability distributions; for example, $P(\theta > 2|D, \alpha) = \int_{2}^{\infty}p(\theta|D\,\alpha)\,\text{d}\theta$ is the probability that a parameter is greater than $2$. For proteomics, these could be the probability that a fold-change exceeded a certain value or the probability that a spectra belongs to a particular peptide. 
\begin{figure}[H]
	\centering
	\includegraphics[width =1\textwidth]{figure1distribution.pdf}
	\caption{\textbf{Example probability distributions.} The Normal distribution (left) and $\log$ Normal distribution (centre) with mean and log-mean parameter 0 and variance and log-variance parameter 1. (right) a Hierarchical distribution defined by log normal distribution with log-variance 1 and log-mean distributed as a normal distribution with mean 0 and variance 1.}.
	\label{figure::figure1dist}
\end{figure}

\subsection{Bayesian contributions to proteomics}
\subsubsection{Bottom-up proteomics and differential abundance}
Next, we discuss Bayesian approaches already applied to proteomics. We focus on proteomics data generated via mass-spectrometry but refer to contributions for the reverse-phase-protein-array (RPPA) literature \citep{Crook::2019, Ni::2019, Maity::2020} and 2D gel electrophoresis \citep{Morris::2011}. \textcolor{red}{These fields are typically interested in testing for differences between biological samples due to a perturbation of interest. Using Bayesian statistics one could model these changes and quantify differences using the probability of a particular fold-change.} A number of approaches have been aimed at quantification and differential abundance analysis of bottom-up proteomics data \citep{Phillips::2021, The::2021, The::2019, Santra::2016, Peshkin::2019, Millikin::2020, Serang::2013, OBrien::2018, Serang::2012, Carvalho::2011}. \citet{Carvalho::2011} describe the use of Bayesian statistics to improve analysis of spectral counting data using a Poisson likelihood and calculating the probability of detecting a protein in a particular sample. However, they do not exploit the full Bayesian toolkit and simply interpret their probabilities as $p$-values. Thus, it is not clear whether this is any better than simply using a likelihood-based method. A number of approaches \citep{The::2021, The::2019, Peshkin::2019, Serang::2012} argue for propagating and quantifying the uncertainty in the analysis rather than simply the underlying quantitation values, either through relaxing the parsimony assumption \citep{Serang::2012}, including ion statistics via a two-level Beta-Binomial model \citep{Peshkin::2019}, or jointly modelling identification and quantitation statistics \citep{The::2019, The::2021}. \citet{Millikin::2020} use an analogue of the t-test and exploit the posterior distribution by using an interval thresholding approach. \citet{OBrien::2018} note the inherent compositional nature of labelled proteomics approaches and include a modelling parameter to model ratio compression allowing better estimation of the true fold changes. Importantly this parameter is shared across all proteins, which allows them to estimate the parameter accurately even for proteins with few observed peptides. \citet{Jow::2014} also model isobaric laballed mass-spectrometry data but do not model ratio compression. Meanwhile for label-free experiments, \citet{OBrien::2018b} explicitly model missingness showing that jointly modelling missingness and abundance leads to improved performance. All of these approaches demonstrate some benefit over previously applied methods suggesting that combining these methods would provide further improvements. It also suggests translating these methods to other proteomics techniques would be a fruitful endeavour.

\subsubsection{Protein and peptide identification}
One of the fundamental problems in mass-spectrometry based proteomics is identifying a peptide from a spectra. A spectra can be very noisy and $b-,y-$ ions can be missing which results in a complex observation process. Furthermore, we have prior knowledge of observing particular amino acid sequences and knowledge of the cleavage process. This is an ideal scenario for the application of Bayesian methods. \textcolor{red}{Bayesian statistics could report the probability that a particular amino acid belongs to a particular spectra.} Indeed, a number of approaches have been applied \citep{Chen::2005, Halloran::2016, Lewis::2018, Claassen::2009}. \citet{Chen::2005} used a fairly simple framework to calculate peptide identification probabilities based on peptide coordinates. \citet{Halloran::2016} employed a dynamic Bayesian network in a method called DRIP, allowing for insertions and deletions to the spectra. By modelling possible alignments between theoretical and observed spectra they were able to calculate the most probable peptide match. The authors found that their approach was an improvement over available methods particularly for low resolution MS2 data. \citet{Lewis::2018} took a different approach to the same problem, incorporating a scoring function into a likelihood model. Their model also allowed deletions directly via indicator functions. Insertions were characterised by excessive deviations from the spectra of the candidate peptide, where excessive is characterised probabilistically via laplace noise. The authors also included prior information about possible cleavage pairs, as well as prior information about the probability of observing a particular peptide sequence in the dataset. Finally, in constrast to \citet{Halloran::2016} they made full use of Bayesian methods and provide a posterior distribution over possible peptides and parameters. This could have allowed multiple peptides to be associated to a spectrum with differing certainty which could have been used in downstream analysis. \citet{Claassen::2009} tackled a slightly different problem and used a non-parametric Bayesian model to predict the coverage in sequential LC-MS/MS experiments but suggest their approach could also have been adapted to database searching and de novo sequencing. Again, these approaches are all shown to have benefits over previously applied methodology. The clearest is the inclusion of more information and the ability to provide a flexible, and well rationalised, model to the underlying data. The ability to exploit uncertainty captured by the posterior distribution for downstream analysis is far more insightful than simply point estimates from a Bayesian analysis. However, these approaches have not yet been widely adopted by the community. This could be because the methods are difficult or expensive to apply, or the benefits are not compelling. We aim to show throughout this review benefits of a Bayesian analysis are compelling and straightforward to obtain.

\subsubsection{Proteoforms and post-translation modifications}
A number of approaches are interested in applications to proteoform analysis (splice isoforms) or post-translational modifications \citep{Chung::2013, Webb::2014, Lim::2017, Shteynberg::2019, Mallikarun::2020} \textcolor{red}{and Bayesian modelling in these fields could answer whether a peptide was modified or not and the localisation of that modification}. \citet{Chung::2013} employed a non-parametric mixture model to jointly model the modification mass for each PTM group and the true (unobserved) location of the modified amino acid. Their approach outperformed other approaches, is fully automated and provides modification confidence scores. However, the approach did not model the underlying spectrum, which could have resulted in unnecessary false positives. \citet{Webb::2014} tackled the proteoform problem by deconvolving peptides into signatures even if they are associated with the same protein. However, they only used a Bayesian point estimate rather than exploiting the full posterior distribution. \citet{Lim::2017} used a Bayesian model to estimate the phosphorylation stoichiometry using Bayesian statistics. By incorporating a physically plausible model they removed problems with previous models that could have allowed negative stoichiometry. Their joint model allowed them to borrow power across replicates and they reported downstream uncertainty. \citet{Shteynberg::2019} use a Bayesian mixture model to compute probabilities for modification sites. This allowed them to combine precomputed scores in a rational way but, again, they did not examine the full posterior distributions. \citet{Mallikarun::2020} employed a Bayesian linear regression modelling strategy to analyse differential PTM data, suggesting their approach outperformed other methods and could allow uncertainty in missing values. The main benefit here appeared to be the regularisation of the parameters using priors rather than specifically the uncertainty quantification in the analysis.

\subsubsection{Biomarkers and clinical proteomics}

Protein biomarkers, molecular indicators of aberrant processes or disease, and clinical proteomics are a key component of proteomics research. For a review of Bayesian method development in biomarker discovery, see \citet{Hernandez::2015}. \textcolor{red}{In these fields, Bayesian statistics can simultaneously model protein abundance levels and the contribution of exogenous variables, allowing researchers to disentangle disease related variability versus variability due to environmental factors.} \citet{Morris::2006, Morris::2008} developed Bayesian wavelet-based functional mixed models for mass-spectrometry-based proteomics data. Their advanced framework, allowed the simultaneous use of nonparametric fixed and random effects, which facilitated adjustment for clinical and experimental covariates that could affect the intensity and location of a spectra. Working with posterior distributions they were able to compute important quantities such as the probability of intensity changes for fixed fold levels and were able to control a Bayesian false discovery rate; that is, the posterior probabilities are thresholded to control the error rate. \citet{Liao::2014} combined that framework with image analysis methods to enable biomarker discovery from LC-MS data. \citet{Hwang::2008} developed a pipeline, MS-BID, for biomarker analysis which uses a Bayesian analysis of variance (ANOVA). \citet{Harris::2009} applied a Bayesian hierarchical linear probit regression (regression where only two outcomes are allowed) model to determine discriminative biomarkers from mass spectrometry data. They found their approach improved over a simple K-nearest neighbour method. Furthermore, by using posterior probabilities they were able to determine which samples will be the most promising for prognostics. \citet{Kuschner::2010} demonstrated a Bayesian network to perform feature selection from mass-spectrometry data. The selected features then provided excellent predictive power. Though again this approach still used a Bayesian point estimate and instead of full posterior distributions. \citet{Deng::2007} developed a Bayesian network which allows them to integrate mass-spectrometry and microarray data, allowing them to borrow power between mRNA and protein levels. Here, they made use of the flexibility of Bayesian statistics to incorporate different modalities and weigh up the uncertainty between different datasets. More recently \citet{Liu::2020} developed Bayesian Function-on-Scalar quantile regression for mass-spectrometry data. This approach noted that biomarker difference may not be apparent at mean regression but rather at a particular quantile (such as the 0.95 quantile). It simultaneously accounted for the functional nature of MALDI-TOF data, incorporated prior knowledge for adaptive regularization and a basis representation which allowed borrowing of power. They found their method identifies biomarkers overlooked by mean regression. 

Bayesian methods for biomarker and clinical proteomics are more developed than other examined proteomics sub-fields with several exemplary methods that make full use of the flexibility of Bayesian modelling and the rich output of the posterior distribution. 
\subsubsection{Chromatography}
To facilitate peptide identification in mass-spectrometry a liquid-chromatography step is usually applied. The time at which a peptide elutes from the liquid chromatography, called the retention time, can be used as additional information to help identify peptides. However, there is uncertainty in this retention time and they can vary from one run to another. \textcolor{red}{Bayesian models could capture the uncertainty in these data, which could be better used to align data between runs.} \citet{Chen::2019} developed a Bayesian model called DART-ID, which models a latent (unobserved) global retention time alignment. This alignment allowed them to combine the outputted posterior error probability of MaxQuant with the inferred RT density in each experiment. Hence, by using this result they updated their confidences and improved coverage in experiments by 50$\%$. Whilst their approach is powerful, they only used a point estimate and obtained uncertainty through bootstrapping. \citet{Maboudi::2017} are interested in the uncertainty in peptide retention time measurements. Using a Gaussian process regression method they were able to accurately predict retention times and obtain uncertainty estimates. They then used the posterior distribution from the regression analysis as a variable retention time window to identify potentially incorrect peptides. This improved over fixed windowing strategies. One potential strategy for improvement, in a similar vain to DART-ID, would have been to update the identification probabilities based the deviation probability from the predicted retention time. This approach naturally fits within a Bayesian framework. 

\subsubsection{Intact, top-down and structural proteomics}
 \textcolor{red}{Bayesian statistics could have a number of uses in the fields of intact, top-down and structural proteomics. For example, an ensemble of structures concordant with the data, along with their relative probabilities, could be reported rather than simply a single estimate.} \citet{Saltzberg::2017} proposed a Bayesian model to resolve residue level information from hydrogen-deuterium exchange mass-spectrometry. They chose uninformative priors, and though they performed inference using Monte-Carlo methods they did not use the posterior distribution. Furthermore, they do not justify why there model allows for negative deuterium incorporation, which maybe arises from a misunderstanding of the positivity constraint induced by their exponential likelihood model. Proteoform analysis is one of the key challenges in top-down proteomics. \citet{Leduc::2014} introduced a C-score, not be confused with a C-statistic, to facilitate automated identification and characeterisation of proteoforms from top-down proteomics data. Ultimately, their approach allowed them to rank probable proteoforms having observed their data. Performing an analysis in a Bayesian framework allowed them to specify a generative model, provide expert prior information and carefully model the underlying noise distribution. Their proposed C-score is essentially a transformed posterior error probability. However, despite their Bayesian framework, they opted for a point estimate of their model, which could have been greatly enhanced by examining the full posterior of their model. \textcolor{red}{A point estimate of probability distribution is a good way to summarise it visually. However, it does not completely characterize that distribution; for example, summarising a normal distribution by its mean is insufficient to describe it. For complex, high-dimensional distributions it is often challenging to find a single visual representation because diagrams can usually only represent three quantities simultaneously. One way to represent uncertainty in this problem is to display the probability of one amino-acid proceeding another using a 20 by 20 matrix, where each entry of the matrix is a probability of observing that order pair of amino-acids (see supplementary materials). This matrix can also be expanded to represent modifications or non-canonical amino acids. A more direct representation of a probability distribution of a spectrum is to use a contour plot in the m/z-Intensity coordinates. Here, the z dimension corresponds to the probability of observing that m/z-Intensity pair. An example for the MS1 spectra of PARRDABZJARA is visualised in the supplementary materials. } \citet{Marty::2015} proposed a Bayesian deconvolution algorithm for Ion Mobility spectra, which was extended in \citet{Kostelic::2021}. Their approach allowed the convolution of the charge distribution with the peak shape to obtain a flexible deconvolution approach. The wide extent of their applications demonstrated the clear benefits of their method. However, their approach also used a point estimate from their analysis. Hence, apart from the use of prior information, it is not clear what particular benefit a Bayesian analysis had for their approach.

\subsubsection{Functional proteomics}       
Functional proteomics methods aim to decipher protein-function on a system-wide scale. \textcolor{red}{ Indeed, Bayesian statistics could be used to quantify the probability of two or more proteins interacting.} One approach is spatial subcellular proteomics \citep{Geladaki::2019, Christopher::2021} where protein are localised to their subcellular niche using mass-spectrometry data. Bayesian approaches have been developed for biochemical fractionation-based subcellular proteomics \citep{Crook::2018, Crook::2019, Crook::2019b, Crook::2020, Crook::2021}. \citet{Crook::2018, Crook::2019, Crook::2019b} demonstrated Bayesian modelling can quantify uncertainty in protein subcellular localisation and identify cases where this may correspond to multi-localising proteins. \citet{Crook::2018} showed that a even a Bayesian point estimate may overlook these cases and more information is obtained by examining the full posterior distribution. \citet{Crook::2020} allowed the uncertainty in the number of subcellular niches to be accounted for and showed that allowing additional niches can be uncovered. However, the model appeared sensitive to the prior choices and should be chosen carefully. \citet{Crook::2021} built on these experiments to analyse differential localisation experiments showing that modelling uncertainty improved power and interpretation compared with other methods. This fully Bayesian analysis; however, is computationally intensive as it attempts to model many datasets at once. Another functional approach is affinity purification mass spectrometry (AP-MS), which allows us to determine protein interactions and complexes \citep{Christopher::2021}. \citet{Choi::2010} developed a non-parametric Bayesian model to bi-cluster AP-MS data. They sampled from the posterior distribution and are hence able to report the uncertainty in the clustering. However, their nested model assumed that the conditional on the Bait cluster the Prey clusters are independent and their model assumed exchangability (permutation leads to the same probability distribution) of the rows and columns. \citet{Fang::2021} proposed a semi-parametric model for thermal protein profiling after identifying proteins that deviate from classic sigmoid behaviour. Semi-parametric models combine interpretable parametric models with more flexible non-parametric models. Using Bayesian analysis they critically assessed the semi-parametric and parametric model fits and demonstrate those proteins that are better modelled by the semi-parametric model share functional enrichments. Again this fully Bayesian approach had demanding computational requirements, which may explain why many methods choose not to employ Bayesian methods.
	
\subsection{The Bayesian workflow}
\subsubsection{Motivating example}
To illustrate the Bayesian workflow, we examine some recently introduced proteomics data generated using the 
orthogonal organic phase separation (oops) method of \citet{Queiroz::2019}. This method is able to efficiently enrich for RNA-binding proteins and hence, by adapting to the dynamic setting, is able to examine
differential RNA binding. This is where the proportion of a particular protein bound to RNA changes depending on the condition. Here, we examine an experiment where thymidine-nocodazole was used to induce cellular arrest. Total and oops-enriched protein abundances where obtained at $0,6$ and $23$ hours post treatment. Each experiment was performed in triplicate, except for at $6$h when four replicates were taken. The 10 total and 10 oops samples were labelled using 2 separate TMT 10-plex kits and quantitative mass spectrometry was performed in two runs using SPS-MS3 on an Orbitrap Fusion Lumos. Here, we attempt to use the Bayesian toolkit to model this data and answer questions about changes in RNA-binding. A heatmap of the data is shown in figure \ref{figure::figure3}. A protein was chosen at random to illustrate the modelling process, NCAPD2, a regulatory subunit of the condensin complex. NCAPD2 is known to have differential subcellular localisation throughout the cell-cycle \citep{Schmiesing::2000}. 

\begin{figure}[H]
	\centering
	\includegraphics[width =1.1\textwidth]{oopsheatmap.pdf}
	\caption{\textbf{Exploratory data analysis of OOPS.} A heatmap of the mass-spectrometry data generated by the oops experiment. The tree clustering is produced using Ward's method. Each cell represents $z$-score normalised protein abundances. Column annotations are encoded as $X{\text{time}}\_{\text{replicate number}}\_{\text{sample type}}$}.
	\label{figure::figure1}
\end{figure}

\subsubsection{Generative modelling}
Having highlighted the successes and limitations of some of the contributions of Bayesian methods to mass spectrometry-based proteomics, below we outline the Bayesian workflow to facilitate it for proteomics. The first tension of Bayesian analysis is the pairing of the likelihood \textcolor{red}{(the model of the data)} and the prior \textcolor{red}{(the model of the parameters of the likelihood)} \citep{Gelman::2017, Gelman::2020, Betancourt::2021}. On one hand, the word \textit{prior} suggests it must be chosen first; however, without knowledge of the likelihood it makes little sense to start selecting priors - we  may not even know the parameters of the model. Thinking of the likelihood and prior as a pair reduces this conceptual tension. It also leads to an explicit way to check our modelling assumptions via generative and predictive modelling \citep{Betancourt::2021}. A generative model generates data consistent with the data. \textcolor{red}{ In this same way that a cake recipe, when the steps are followed, generates a cake.} The prior has good predictive properties if the \textit{posterior predictive distribution} can predict new data generated from similar experiments. To be explicit, given a likelihood and prior, we can simulate data $y$. First, sample the parameters of the likelihood, $ L(\theta|D)$, from the prior, $p(\theta|\alpha)$, and then given these parameters sample data from the model:	
\begin{equation}
\begin{split}
\tilde{\theta} &\sim p(\theta|\alpha) \\
\tilde{y} & \sim p(y|\tilde{\theta}).
\end{split}
\end{equation}
This leads us to define the \textit{prior predictive distribution}:
\begin{equation}
p(\tilde{y}|\alpha) = \int_{\theta} p(\tilde{y}|\theta)p(\theta|\alpha)\, \text{d}\theta.
\end{equation}
\textcolor{red}{For example assume data are modelled as a log normal distribution with parameter $m$ and $\sigma = 1$. Assume the prior on $m$ is a normal distribution with mean $0$ and variance $1$. The prior predictive distribution first samples $m_i$ from $\mathcal{N}(0,1)$ and then $\log y_i \sim N(m_i, 1)$. In fact, this is the hierarchical distribution shown in figure \ref{figure::figure1dist}}. There are a number of key observations. Firstly, the prior predictive distribution has no knowledge of the data, aside from the modelling assumptions of the domain expert. Secondly, the likelihood and prior are now explicitly coupled and so poor modelling choices in either the likelihood or prior will be apparent via the prior predictive. Thirdly, the failure of uniform or uninformative priors as a default is clear, as they will generate unrealistic data.

In our oops example, we model $\log$ protein abundance as a linear model of sample type (whether total or oops) and time ($0,6,23$h). Since we are interested in changes in the proportion of protein bound to RNA, we include an interaction effect between time and sample type. We then use Gaussian priors on the coefficients of the effects and an exponential prior on the standard deviation of the Gaussian noise. Formally, the model can be written as
\begin{equation}
\begin{split}
\log y &= \beta_{type} + \beta_{time} + \beta_{time:type} + \epsilon\\
\epsilon &\sim \mathcal{N}(0, \sigma^2)\\
\beta &\sim \mathcal{N}(0, 1)\\
\sigma &\sim \text{Exp}(1).
\end{split}
\end{equation}
The priors were chosen arbitrarily and we can use a prior predictive check to see whether this leads to a sensible generative model. Figure \ref{figure::figure2} show a variety of prior predictive checks using different summaries of our observed and simulated data. We see that the our generative model is too diffuse compare with the observed data and produces large deviations beyond what we would expect from a typical proteomics dataset. Hence, it is necessary to explore more prior choices using prior predictive checks. In the accompanying vignette, we show that our inferences can be better calibrated using an exponential prior with rate $4$, which corresponds to $1$ in $5000$ proteins having a standard deviation in their $\log$ abundance above $2.5$.

\begin{figure}[h!]
	\centering
	\includegraphics[width =1\textwidth]{priorpredictvechecks1.pdf}
	\caption{\textbf{Prior predictive checks.} Prior predictive checks applied to oops data. $y$ denotes the observed data, whilst $y_{rep}$ denotes the simulated data from the prior predictive distribution. (a) kernel density estimation based checks (b) Histogram based checks (c) boxplot based checks (d) summary statistics checks (e) interval plot based checks (f) error histogram based checks. This figure can be reproduced in the vignette and evaluated for other prior choices.}
	\label{figure::figure2}
\end{figure}


\subsubsection{Predictive modelling}
Once our prior and likelihood have seen the data, $D$, they are updated into the posterior distribution. We can then sample new data by first sampling parameters from the posterior distribution and then again sampling from the likelihood:
\begin{equation}
	\begin{split}
		\tilde{\theta} &\sim p(\theta|D, \alpha) \\
		\tilde{y} & \sim p(y|\tilde{\theta}).
	\end{split}
\end{equation}  
This leads to the definition of the posterior predictive distribution:
\begin{equation}
	p(\tilde{y}|D, \alpha) = \int_{\theta} p(\tilde{y}|\theta)p(\theta|D, \alpha)\, \text{d}\theta = \int_{\theta}p(\tilde{y}|\theta) \frac{p(D|\theta, \alpha)p(\theta|\alpha)}{p(D|\alpha)} \text{d}\theta.
\end{equation}
We have expanded the integrand using Bayes theorem to make a key point explicit: the posterior predictive distribution depends on the likelihood, the prior and the data. This coupling allows us to make a number of observations. A good choice of prior and likelihood leads to good predictive performance and over-fitting can be examined via the posterior predictive distribution. 

Having fitted the model to the data, we can perform a posterior predictive check on our inferences. Figure \ref{figure::figure3} shows a number of posterior predictive checks and that clearly the model has learnt from the data. Visualisation show that samples from the posterior predictive distribution look similar to the observed data. Contrast this with prior predictive checks where the samples from the distribution were very diffuse.

\begin{figure}[h!]
	\centering
	\includegraphics[width =1\textwidth]{posteriorpredictivechecks.pdf}
	\caption{\textbf{Posterior predictive checks.} Posterior predictive checks applied to oops data. $y$ denotes the observed data, whilst $y_{rep}$ denotes the simulated data from the prior predictive distribution. (a) kernel density estimation based checks (b) Histogram based checks (c) boxplot based checks (d) summary statistics checks (e) interval plot based checks (f) error histogram based checks. This figure can be reproduced using the vignette and more choices can be explored.}
	\label{figure::figure3}
\end{figure}

\subsubsection{Fitting a model: Bayesian computation}
In practice, the integrals and probability distribution required for sufficiently flexible modelling are intractable. We can perform inference in a wide array of models using Markov-chain Monte Carlo (MCMC) methods \citep{Gilks::1995, Brooks::2011}: including Gibbs sampling \citep{Smith::1993, Gelfand::2000}, Metropolis sampling \citep{Robert::1999}, and Hamiltonian Monte Carlo\citep{Hoffman::2014}. Bayesian inference can also be performed using sequential Monte Carlo \citep{Del::2006} or variational inference \citep{Blei::2017}. Although the latter can provide a fast approximation of the posterior distribution, it can be arbitrarily inaccurate. Here, we focus on Hamiltonian Monte Carlo, as it forms the basis of modern probabilistic programming languages \citep{Carpenter::2017}. 

Initially, when an MCMC algorithm begins it will "move" towards the posterior distribution producing a "sample" at each iteration. An initial warm-up or burn-in section is require to remove bias due to dependence of the algorithms starting values and to adapt some of the algorithms tuning parameters to provide efficient inference. Once the warm-up section is complete, there is a sampling period which is run until multiple chains have mixed \textcolor{red}{(sampling from the same posterior distribution)}. One measure of mixing chains is $\hat{R}$, which is essentially a measure of between and within chain variance \citep{Vehtari::2019}. Current standard practise is that $\hat{R}$ should be close to $1$. It is also recommend to visualise trace plots and rank histograms for samples from an MCMC algorithm \citep{Gabry::2019}. Some tools include further diagnostic checks such as divergences but this is beyond the scope of this review \citep{Betancourt::2017}. Table \ref{table::bayesian computation} highlights some probablistic programming languages that can be used to fit general purpose Bayesian models. For our oops example Bayesian computations are reliable, see accompanying vignette and the supplement.

\begin{table}[ht]
	\centering
	
	\begin{tabular}{ |p{3cm}||p{2cm}|p{7cm}|p{5cm}|  }
		\hline
		\multicolumn{4}{|c|}{Packages for Bayesian computation} \\
		\hline
		Computational tool & language & Inference method & Reference\\
		\hline
		stan   & c++   &HMC variant&   \citep{Carpenter::2017}\\
		brms &   R  & HMC variant   & \citep{Burkner::2017}\\
		MCMCglmm & R & Metropolis/Slice sampling &  \citep{Hadfield::2010}\\
		PyMC3    & Python & HMC variant&  \citep{Salvatier::2016}\\
		BUGS &   BUGS/R  & Gibbs sampling & \citep{Lunn::2009}\\
		Edward&  Python  & Various including variational inference   &\citep{Tran::2016}\\
		Pyro & Python & HMC variant& \citep{Bingham::2019}\\
		Turing.jl & Julia & Various including HMC & \citep{Ge::2018}\\
		\hline
	\end{tabular}
\caption{\textbf{General purpose probabilistic programming languages.} A variety of probabilistic programming languages are available in several languages using modern and efficient inference methods. Amongst these languages, one can fit the vast majority of models used in practice.}
\label{table::bayesian computation}
\end{table}
\subsubsection{Posterior z-scores and contraction}
It is often desirable to evaluate the behaviour of a model, and if any model assumptions are preventing us from making sensible inferences. The \textit{posterior z-score} and \textit{posterior contraction} are useful metrics to identify several problems with a model \citep{Betancourt::2021}. Let's assume, we have access to a parameter, $\theta^*$, of the true data generating process. The \textit{posterior z-score} for a parameter is defined as:
\begin{equation}
z_{\text{post}}(\theta|\tilde{y}, \theta^*) = \frac{E_{\text{post}}[\theta|\tilde{y}] - \theta^*}{s_{\text{post}}(\theta|\tilde{y})},
\end{equation}
where $E_{\text{post}}$ denotes the expectation under the posterior and $s_{\text{post}}$ the standard deviation under the posterior. The \textit{posterior contraction} is defined as
\begin{equation}
c(\theta|\tilde{y}) = 1 - \frac{V_\text{post}(\theta|\tilde{y})}{V_\text{prior}(\theta|\tilde{y})},
\end{equation}
where $V_\text{post/prior}$ denotes the variance under the posterior/prior. Together these quantities tell us about how the posterior is learning from the data. If the posterior z-score is large and the posterior contraction is small, then the prior modelling conflicts with the true process - we are unable to learn the true parameter well. If the posterior z-score is large and the posterior contraction is close to $1$, this suggest we are concentrating on an incorrect part of the probability space and so the model is over-fitting. If the posterior z-score is small and the posterior contraction is also small then the model is poorly informed by the data. The ideal scenario is that posterior contractions are close to $1$, and that posterior z-scores are close to $0$. This tells us that the data is highly informative and the prior was not biased away from the data generating mechanism. Examples of posterior contractions are shown in the accompanying vignette.

\subsubsection{Model selection and averaging}
Using probability allows us to select between competing models that may generate the data. \textcolor{red}{ For example, was this model generated by data with two or three clusters/groups?} Given two models, $\mathcal{M}_1$ and $\mathcal{M}_2$, we can ask for the $P(D| \mathcal{M}_i)$ for $i = 1,2$. The relative plausibility of two models is referred to as the \textit{Bayes factor} \citep{Kass::1995},
\begin{equation}
\text{BF}_{12} = \frac{P(D|\mathcal{M}_1)}{P(D|\mathcal{M}_2)} = \frac{P(\mathcal{M}_1|D)}{P(\mathcal{M}_2|D)}\frac{P(\mathcal{M}_2)}{P(\mathcal{M}_1)}. 
\end{equation}   
The Bayes factor allows an interpretable and quantitative way to evaluate the relative plausibility of two models, by examining the ratio of the probabilities of the model generating the data. \textcolor{red}{ The Bayes factor is the ratio of the posterior probabilities of the models multiplied by the ratio of the prior probabilities of the models. As such, it incorporates the plausibility of the models before we saw any data, with the plausibility of the data after we have seen the data}. From a brief calculation, we can see that:
\begin{equation}
P(D|\mathcal{M}_1) = \int p(D|\theta_1, \mathcal{M}_1)p(\theta_1|\mathcal{M}_1)\, \text{d}\theta_1,
\end{equation}   
where $\theta_1$ are parameters that parametrise model $\mathcal{M}_1$. Here, we see the dependence of the Bayes Factor on the prior and the implicit assumption that we are evaluating models on their prior predictive performance becomes explicit. Thus, using improper/uninformative priors with Bayes factor would be inappropriate. However, there a further complexities, the most concerning perhaps is that one can inflate the Bayes factor by simply choosing a prior that places probability on unrealistic parts of the parameter space. Typically a uniform prior would have such an effect. Thus if you are unsure of the veracity of your prior choices, model evaluation may be better using functions of the posterior predictive distributions \citep{Betancourt::2021}. 

We have already seen that one of the key mechanics of Bayesian statistics is the ability to average over quantities, rather than simply taking the best parameters forward. This can also be performed with models using so-called Bayesian model averaging \citep{Raftery::1997}. Let $\phi$ be a quantity of interest \textcolor{red}{(such as the abundance of a protein from an experiment)} and given models, $\mathcal{M}_i$ $i = 1,..., n$ \textcolor{red}{(such as different plausible regression models)}, we may average them:
\begin{equation}
p(\phi|D) = \sum_{i = 1}^{n}p(\phi|D, \mathcal{M}_i)p(\mathcal{M}_i|D).
\end{equation}
This is the average of the posterior predictive distribution for $\phi$ under the models considered, weighted by their posterior model probability. If we are interested in the Bayesian model average estimate of a particular parameter, we can compute
\begin{equation}
\hat{\theta} = E_{\text{BMA}}[\theta] = \sum_{i = 1}^{n}E_{\mathcal{M}_i}[\theta_i] p(\mathcal{M}_i|D).
\end{equation}
Given the sensitivity of the Bayes factor to the prior, it is sometimes useful to consider model selection based on the posterior predictive distribution. One example is the log pointwise predictive density (lpd) \citep{Vehtari::2017}:
\begin{equation}
\text{lpd} = \sum_{i = 1}^n \log p(y_i|D) = \sum_{i = 1}^n \log \int p(y_i|\theta)p(\theta|D)\, \text{d}\theta.
\end{equation}
\textcolor{red}{This is a measure of the total probability of the observed data as if it was being predicted from having fitted a Bayesian model.} Furthermore, it is frequently useful to consider an out-of-sample predictive fit via leave-one-out (LOO) cross-validation \citep{Vehtari::2017}:
\begin{equation}
\text{lpd}_\text{LOO} = \sum_{i = 1}^{n}\log p(y_i| D_{-i}),
\end{equation}
where $D_{-i}$ is data without data point $i$. This quantity can be efficiently approximated using the LOO package \citep{Vehtari::2017}. \textcolor{red}{This is the same as the above quantity except that we have hidden $y_i$ from the model and so it measures the ability of the model to generalise to unseen data.} We note that the above definitions can be adapted to any utility or loss function so that the metric of interest can be characterised.

\textcolor{red}{Returning to our oops example, in our second vignette we develop more complex models of the data. These models include group-level (random) effects for the replicate number and TMT tag used (see model strategies section). Our oops example is designed as a typical proteomics experiment where the time points include three or four biological replicates. Proteomics experiments usually have nested hierarchical levels of replicates; for example, biological or technical/injection replicates. Estimating the variability associated with replication typically improves power to detect differences between samples. Including such effects are typically straightforward in a Bayesian analysis through the use of group-level parameters as highlighted in our example below. All the software packages in table 1 can be used to construct such models. } The three competing models are
\\
Model 1:
\begin{equation}
	\begin{split}
		\log y &= \beta_{type} + \beta_{time} + \beta_{time:type} + \epsilon\\
		\epsilon &\sim \mathcal{N}(0, \sigma^2)\\
		\beta &\sim \mathcal{N}(0, 1)\\
		\log \sigma &=  \beta_{\sigma, time} + \beta_{\sigma, type} \\
		\beta_{\sigma} &\sim \mathcal{T}(3, 0 , 1).
	\end{split}
\end{equation}
Model 2:
\begin{equation}
	\begin{split}
		\log y &= \beta_{type} + \beta_{time} + \beta_{time:type} + u_{replicate} +  \epsilon\\
		\epsilon &\sim \mathcal{N}(0, \sigma^2)\\
		\beta &\sim \mathcal{N}(0, 1)\\
		\log \sigma &=  \beta_{\sigma, time} + \beta_{\sigma, type}\\
		\beta_{\sigma} &\sim \mathcal{T}(3, 0 , 1)\\
		u_{replicate} &  \sim\mathcal{N}(0, \sigma^2_{replicate})\\
		\log \sigma_{replicate} & \sim \mathcal{T}(3, 0 , 0.1)
	\end{split}
\end{equation}
Model 3:
\begin{equation}
	\begin{split}
		\log y &= \beta_{type} + \beta_{time} + \beta_{time:type} + u_{replicate} + u_{TMT} +  \epsilon\\
		\epsilon &\sim \mathcal{N}(0, \sigma^2)\\
		\beta &\sim \mathcal{N}(0, 1)\\
		\log \sigma &=  \beta_{\sigma, time} + \beta_{\sigma, type}\\
		\beta_{\sigma} &\sim \mathcal{T}(3, 0 , 1)\\
		u_{replicate} & \sim \mathcal{N}(0, \sigma^2_{replicate})\\
		u_{TMT} & \sim \mathcal{N}(0, \sigma^2_{TMT})\\
		\log \sigma_{replicate} &\sim \mathcal{T}(3, 0 , 0.1)\\
		\log \sigma_{TMT} &\sim \mathcal{T}(3, 0 , 0.1)
	\end{split}
\end{equation}
Each of the models progresses with more complexity. We compute the posterior model probabilities for each of these examples and find that $P(\mathcal{M}_1|D) = 0.35$, $P(\mathcal{M}_2|D) = 0.48$ and $P(\mathcal{M}_3|D) = 0.17$ (see vignette for more details). This suggest that a group-level effect for replicate is warranted but there is less support for the more complex model 3. Note that because computing the posterior model probabilities includes integration against the prior, these probabilities are automatically penalised for model complexity. See accompanying vignette for further exploration. 

\subsubsection{Using uncertainty from a Bayesian analysis}
Bayesian's quantify uncertainty using probability distributions. Perhaps the most commonly used representation of uncertainty is the credible interval \citep{Gelman::1995}. A credible interval is an interval $(a,b)$ such that a parameter lies within this interval with some probability. For example, we could ask for an interval such that the probability that a protein's $\log$ abundance falls between $a$ and $b$ with probability $0.95$. In notation used earlier $P(a < \log x < b) = 0.95$. We can see that the interval $(a,b)$ is not unique. 

The analogous quantity in frequentist statistics is the confidence interval; however, it is an entirely different concept. This is seen most clearly by asking which part of the constructions are random. For credible intervals, it is the quantity of interest $\theta$ that is random and the interval that is a fixed quantity. Whilst, for a confidence interval the parameter is fixed and the interval is random, since it depends on the randomly observe sample. 

However, Bayesian's can report any quantity that can be derived from the posterior distribution or posterior predictive distribution, which in practice can very complex representation of uncertainty. Since summarisation can distort the representation of uncertainty, we recommend reporting the full posterior distribution whenever that is practical.

For our oops example, we are interested in the interaction effects, since these allow us to determine whether the proportion of protein bound to RNA is changing between conditions. In figure \ref{figure::figure4}, we plot the joint distribution of the two interaction effects. We can then ask a number of question of this joint distribution. Some examples include the probability of being positive or negative, the probability of having the opposite signs, the probability that the absolute effects are exceed $0.1$, and many more (see accompanying vignette). \textcolor{red}{One possible deduction from figure \ref{figure::figure4} is that the probability that the interaction effect changes sign from negative to positive is $0.779$. This means that it is likely that the proportion of protein bound to RNA is depleting at 6h whilst it is increasing at 23h. This suggests that this RNA-binding protein is functionally relevant during the cell cycle.}

\begin{figure}[h!]
	\centering
	\includegraphics[width =1\textwidth]{figure4.pdf}
	\caption{\textbf{Joint posterior distribution of interaction effects.} Joint posterior distribution of the interaction effect of type with time at six hours and twenty-three hours. The distribution is shown as a $2$d histogram with hexagon based density estimation. Orange regions highlight absolute effect sizes less than $0.1$. We observe that the density is concentrated outside this region, though is more overlapping at twenty-three hours.}
	\label{figure::figure4}
\end{figure}


\subsection{Modelling strategies}
\subsubsection{Parametric models}
Here, we outline some commonly used modelling strategies and relate them to the proteomics literature. This is not meant to be exhaustive, nor could it be, since there are infinitely many possible models one could specify. One of the most commonly used models is the linear model, where we wish to link a set of predictor to outcomes:
\begin{equation}
y = \beta X + \epsilon.
\end{equation}
If we choose $\epsilon$ to be Gaussian noise, we can write down the model as follows:
\begin{equation}
y \sim \mathcal{N}(\beta X, \sigma^2).
\end{equation}
There is nothing Bayesian about this model until we specify priors. Remember, the choice of prior should be motivated by generative and predictive modelling and of course the priors should respect the domain of the parameters. Typically, one may start with a Gaussian or Student-t prior on $\beta$. The prior on $\sigma$ could be specified from a variety of probability distributions that respect positivity. Usually recommendations include half-normal, exponential, half-student-t and half-Cauchy depending how confident we are about the scale of the noise \citep{Gelman::2017}. Since protein abundances are positive quantities, it is typical to model them as a log normal distribution
\begin{equation}
\log y  \sim \mathcal{N}(\beta X, \sigma^2).
\end{equation}   	
If our observed data were counts then it maybe sensible to use Poisson or Negative binomial regression \citep{Lawless::1987}:
\begin{equation}
\begin{split}
y &\sim \text{Pois}(\lambda)\\
\log(\lambda) &= \beta X\\
\beta &\sim \mathcal{N}(0, \sigma^2),
\end{split}
\end{equation}
and
\begin{equation}
	\begin{split}
		y &\sim \text{NB}(r, p)\\
		\text{logit}(p) &= \beta X\\
		\beta &\sim \mathcal{N}(0, \sigma^2).
	\end{split}
\end{equation}
In each of the above cases, we would have to choose appropriate priors on the model parameters. Again, using the evaluation strategies previously discussed. Many data have an excess of zero's which are not captured by the usual statistical models. Many distribution can be extended to hurdle or zero-inflated models to account for these observations. The distribution of the noise process can be, again, as exotic as needed for the task at hand. Consistent outliers might call for student-t distribution or perhaps the noise itself depends on some covariates, such as time or spatial location. See \citet{Goeminne::2020} for an example, albeit non-Bayesian, of a hurdle model applied to proteomics.

Another useful model strategy is to allow parameters at the population-level and group-level, note that these are sometimes referred to as fixed and random effects. For example, a paired t-test is a linear model with grouping specified by the subject or replicate. More complex groupings are allowed, including interactions between groupings and groups that are nested within each other. If $\beta$ and $u$ are population-level and group-level coefficients with design matrices $X$ and $Z$, then a log linear (mixed) model would be \citep{Bates::2014}:
\begin{equation}
\log y = \beta X + u Z + \epsilon.
\end{equation}
\textcolor{red}{Group-level parameters (random/mixed effects models) allow us to model the nested and hierarchical variability that is associated with typical proteomics experiments. For example a proteomics experiment may have biological and technical/injection replicates. The number of replicates performed is arbitrary and thus a replicate is a sample (in the probabilistic sense) from the distribution of replicates. We can then model replicates as if they add noise to a true biological signal $\beta$. Let $\nu_j$ be the noise added by technical replicate $j$ and $\eta_k$ be the noise added by biological replicate $k$. Then the log abundance of protein $i$ in technical replicate $j$ in biological replicate $k$ could be modelled as:
\begin{equation}
\log y_{ijk} = \beta + \nu_j + \eta_k + \epsilon_{ijk}.
\end{equation}
}
As before the flexibility of the Bayesian analysis, allows you to build any sensible probability distribution on top of this initial model. See \citet{Morris::2008, Morris::2011} for application of mixed-models to proteomics data, as well as the accompanying vignette.

Another useful modelling strategy is mixture models, which occurs frequently in the context of clustering and classification \citep{Mclachlan::2019}. The mixture model assume that data arises from different components each with the same parametric density with different parameters:
\begin{equation}
\begin{split}
y_i|z_i,\theta &\sim F(\theta_{z_i})\\
z_i|\pi & \sim \text{cat}(\pi)\\
\pi|\alpha & \sim \text{Dir}(\alpha)\\
\theta & \sim p(\theta).
\end{split}	
\end{equation}
The priors and the likelihood can be chosen based on the specific application at hand and the workflow recommendations can be applied. It is often insightful to write, using the law of total probability, the mixture model as
\begin{equation}
p(y_i) = \sum_{k = 1}^{K} \pi_k p(y_i|\theta_k).
\end{equation}  
Note that because a Dirichlet prior is placed on $\pi$, the entries must all be non-negative and sum to unity. Hence, the entries of $\pi$ can be interpreted as weights. The data cluster by being associated to the component density which fits those observations through the variables $z_i$. Examples of mixture models applied to proteomics include \citet{Chung::2013, Crook::2018, Crook::2019b}

\subsubsection{Non-parametric models}
In contrast to parametric models, non-parametric models allow more parameters as more data is observed. Phrased another way, in a parametric model there are finitely many parameters, whilst in a non-parametric model there are infinitely many such parameters. This makes non-parametric models more flexible; however, to avoid the over-fitting concerns raised in earlier sections, we ought to be prudent with our choice of priors. One of the most popular non-parametric model is the Gaussian process (GP), which can be used to model functions $f$. Suppose we observe data $\left\{(x_i,y_i)_{i = 1,...,n}\right\}$, we wish to find a function $f$ such that $f(x_i)$ models $y_i$. Let us assume a Gaussian regression set-up, using a \textit{Gaussian process prior} to model $f$:
\begin{equation}
\begin{split}
y &\sim \mathcal{N}(f, \sigma^2)\\
f &\sim \mathcal{GP}(m, C).
\end{split}
\end{equation}
The Gaussian process is a distribution over \textit{functions} that is uniquely characterised by its mean and covariance functions. The choice of mean and covariance functions are modelling choices to be made by the domain expert. Typically, the covariance function is parametrised by some parameters $C = C(\theta)$ and we can also place priors on these parameters so that $\theta \sim p(\theta)$. Again, these modelling choices can be evaluated using prior/posterior predictive checks. We refer to several discussion on choose priors for Gaussian processes \citep{Berger::2001,Paulo::2005, De::2007, Van::2009, Fuglstad::2019}. For applications of Gaussian process to proteomics data see \citet{Maboudi::2017,Crook::2019b, Shin::2020, Fang::2021}.

The other non-parametric model that is frequently used is the Dirichlet process \citep{Ferguson::1973, Antoniak::1974}. Dirichlet processes are a popular tool for modelling data with parameter repetitions. For example, when we cluster data, all observation associated with cluster 1 share the same parameter $\theta_1$. The Dirichlet process is defined using a base distribution $G$ and a concentration parameter $\alpha$ and is written $\text{DP}(G, \alpha)$. For example, suppose that $G = \mathcal{N}(0, 1)$, then we can simulate from the Dirichlet process as follows. For any $i \geq 1$, with probability $\frac{\alpha}{\alpha + i - 1}$ sample $x_i \sim \mathcal{N}(0, 1)$ and with probability $\frac{n_x}{\alpha + i - 1}$  let $x_i = x$, where $n_x$ is the number of previous observations of $x$. This means, if we have already observed a value, then we are increasingly likely to observe it in the future. This property is sometime referred to as the "rich get richer property".

The Dirichlet process allows us to work with mixture models with infinitely many components, which is useful for characterising the uncertainty in the number of components. Once we have a sensible parametric likelihood for the observations $F(\theta_i)$, the Dirichlet process can be used as a prior to construct the Dirichlet process mixture model:
\begin{equation}
\begin{split}
y_i|\theta_i &\sim F(\theta_i)\\
\theta_i|P &\sim P \\
P|\alpha, G & \sim \text{DP}(G, \alpha).
\end{split}
\end{equation}
Since $P$ will be discrete, the set$\{\theta_i\}_{i = 1,...,n}$ will contain repetitions. This allows us to think of this model as a mixture model, where the groups of parameters define the components. Extensions are available \citep{Teh::2006, Rodriguez::2008} and MCMC algorithms for fitting these models can be found in \citet{Neal::2000}. For applications of Dirichlet processes to proteomics data see \citet{Claassen::2009} and \citet{Choi::2010}.

\section{Discussion}
Despite Bayesian statistics offering a powerful and flexible framework for performing proteomics data analysis, only a few problems have yet been tackled using this methodology. Even when Bayesian statistics has been applied, the methodology has not made complete use of the information available from such an analysis. Many analysis have simply resorted to proxies from frequentist based approaches. One of the key advantages of the Bayesian approach is to be able to jointly model several quantities and provide uncertainty estimates in any parameters. Another advantage of Bayesian statistics is that it makes modelling assumption explicit; hence, it becomes clear how the models can be improved and what is the extent of their limitations.

Here, we have summarised key modelling ideas in Bayesian statistics starting with the workflow. We have highlighted that the Bayesian workflow has a consistent approach to model building, model criticism and evaluation grounded in probability theory. Using a case study, we have provided a workflow for developing a Bayesian model for Organic Orthogonal Phase Separation (oops) data. We then proceeded to describe and illustrate common modelling strategies to help proteomics researchers understand key models in the literature and link them to current methods used in the literature.

Mass spectrometry-based proteomics appears to have resisted uptake on Bayesian methods for various reasons. These include, but are not limited to, lack of familiarity with the workflow and tools available, lack of compelling examples in literature, and lack of desire to invest in bespoke model development. We hope that this review goes some way in removing some of these barriers to applying and understand Bayesian methods. 

\section{Acknowledgements}
OMC acknowledges funding from GSK and a Todd-Bird Junior Research Fellowship from New College Oxford. CWC is an employee of GSK. 
\section{Data availability}
Data to reproduce the figures is provided in the supplementary material . Experimental data are available from the original manuscripts.
\section{Code availability}
Code is available from the manuscript github repository \url{https://github.com/ococrook/2021-BayesProtReview}
\section{Supplementary material}
\begin{itemize}
\item Supplementary Data S1: oopsdata, Data for oops modelling .
\item Material S1: $\text{oops\_modelling}$, supplementary vignette for data exploration and straightforward modelling.
\item Material S2: $\text{oops\_modelling\_part2}$, supplementary vignette for advanced modelling.   
\end{itemize}
	
\bibliographystyle{natbib}
\bibliography{BayesProtReviewBib}	
\end{document}