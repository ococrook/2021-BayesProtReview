\documentclass[12pt,english]{article}
\usepackage{natbib}
\usepackage{amsmath,mathtools,amssymb,mathrsfs,dsfont,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{color}
\graphicspath{ {figures/} }
\usepackage[colorlinks]{hyperref}
\hypersetup{citecolor=blue}
\usepackage{enumitem}
\usepackage{authblk}
\usepackage{pifont}
\usepackage{lineno}
\usepackage[normalem]{ulem}
\usepackage[export]{adjustbox}
\usepackage{ccaption}
\linespread{1.1}

\renewcommand\Affilfont{\itshape\scriptsize}
\renewcommand\Authfont{\small}
\newcommand{\xmark}{\ding{55}}
%opening
\title{Challenges and opportunities for Bayesian statistics in proteomics}
\author[1]{Oliver M. Crook \thanks{\url{oliver.crook@stats.ox.ac.uk}}~}
\author[2]{Chun-wa Chung}
\author[1]{Charlotte M. Deane}
	
	
\affil[1]{Department of Statistics, University of Oxford, Oxford, UK}
\affil[2]{Structural and Biophysical Sciences, GlaxoSmithKline R\&D, Stevenage, UK}

\begin{document}
\maketitle
\begin{abstract}
Proteomics is a data-rich science with complex experimental designs and an intricate measurement process. To obtain insights from large datasets, statistical methodology and machine learning is routinely applied. For a quantity of interest, many of these approaches only produce a point estimate, such as a mean, leaving little room for bespoke interpretations. In contrast, Bayesian statistics quantifies uncertainty using probability distributions. These probability distributions allow scientist to ask complex questions of their proteomics data which would otherwise be challenging using alternative approaches. Bayesian statistics also offers a modular framework for specifying complex hierarchies of parameter dependencies. This allows us to use statistical methodology which equals, rather than neglects, the sophistication of experimental design and instrumentation present in proteomics. Here, we review Bayesian methods applied to proteomics and argue for a broader uptake, whilst also highlighting the challenges posed by adopting a new statistical framework. To illustrate our review, we present a walk-through of the development of a Bayesian model for dynamic organic orthogonal phase-separation (OOPS) data.      
\end{abstract}
\section{Introduction}
Decision making spans the entire research process. Ultimately, it is a choice to believe an explanation for a phenomena given the current evidence. For some theories, the evidence is overwhelming: careful mechanistic experiments and verifiable model predictions have never contradicted that theory. This scenario is, however, rare. In practice, we make decisions under uncertainty and the evidence is not clear-cut. Bayesian statistics allows us to make inferences from that evidence to enable decision making in those cases. In contrast to \textit{frequentist} methods, Bayesian inference allows us to use probability to model degrees of belief, rather than just frequencies. Consequently, models that are consistent with the available evidence are more probable and incompatible models are less probable. By using probability theory in this manner, there is a recipe for taking \textit{prior beliefs} (i.e. information encoded by domain expertise) and updating them to \textit{posterior beliefs} using observed data. As a result, this posterior probability distribution quantifies the models compatible with domain expertise and our experimental data. This recipe is known more formally as \textit{Bayes' theorem}. 

Mass-spectrometry-based proteomics is a complex scientific field. The techniques versatility allows us to explore differential abundance, protein turnover, interactions, thermal stability, structure, spatial information and more. In each case, data are manipulated, thresholded and filtered so that a statistical test or machine learning algorithm can be applied. The results are then frequently concluded with a single value, which we have granted the role of arbiter of truth. These decisions are often made without consideration of what we might be happening at each step. Bayesian statistics could propagate or quantify the uncertainty in these steps, replace implicit or ad-hoc approaches with explicit models and summarise the output with a probability distribution consistent with our data. This paradigm progression not only provides us an ability to ask new questions of our data but a consistent way to perform inference and criticize our models. 

Bayesian statistics offers proteomics considerable possibilities; despite that, it has not been readily adopted in the community. This may stem from a lack of familiarity, a lack of awareness of available tools, complex language, impenetrable literature, inability to communicate results from an analysis and computational difficulties. Here, we review the contribution Bayesian statistics has already made to proteomics, clarify the Bayesian workflow and how it can be applied to proteomics, highlight a number of modelling strategies and outline current challenges for the community. Throughout, we illustrate our analysis with examples from the proteomics literature, focusing on building a model for dynamic organic orthogonal phase separation data.
\section{Main}
\subsection{Bayes, in brief}
Before we review the contributions Bayesian statistics has already made to proteomics, we introduce the fundamental technical background and notation. We use $P(E)$ to denote the probability of the event $E$. $E$ can be anything from "it rains tomorrow" to "my parameter falls between the values $a$ and $b$". We let $D$ be notation for the observed data, for example from a shotgun proteomics experiment. Let $x$ be a data point from $D$, such as a measurement for a particular protein. We assume that $x$ arises from some probability distribution $p$ and we write $x \sim p(x|\theta)$, for example this could be a log normal distribution. Let $\alpha$ be hyperparameters of the parameter distribution, such that $\theta$ themselves are drawn from a probability distribution $\theta \sim p(\theta|\alpha)$. For example, the mean of the log normal distribution could be drawn from a normal distribution. 

The \textit{prior distribution} captures our domain expertise and is the distribution of the parameters before any data is observed: $p(\theta|\alpha)$. The \textit{prior} could capture, say, that abundance values are positive and are unlikely to exceed the number of grains of sand on Earth. The sampling distribution is the distribution of the data given the parameters $p(D|\theta)$, we can write this as a function $L(\theta|D)$ called the \textit{likelihood}. If we average (or \textit{marginalise}) the distribution of the data over the parameters, we obtain the so-called marginal likelihood:
\begin{equation}
p(D|\alpha) = \int L(\theta|D)p(\theta|\alpha)\,\text{d}\theta.
\end{equation}
The posterior distribution of the parameters is determined by Bayes' theorem, as the following:
\begin{equation}
p(\theta|D, \alpha) = \frac{p(D|\theta, \alpha)p(\theta|\alpha)}{p(D|\alpha)}.
\end{equation}
Bayes' theorem tells us the mathematical way to update beliefs in light of evidence: simply multiply our prior and likelihood and renormalise by the marginal likelihood. Bayes' theorem implies a self-consistency property: the posterior averaged over the data returns the prior:
\begin{equation}
p(\theta) = \int \int p(\theta|\tilde{y})p(\tilde{y}|\tilde{\theta})p(\tilde{\theta})\,\text{d}\tilde{y}\text{d}\tilde{\theta},
\end{equation}
where $\tilde{y} \sim p(D|\tilde{\theta})$. When Bayesian's predict, instead of simply taking a single parameter value forward, they make predictions by averaging:
\begin{equation}
p(\tilde{x}|D,\alpha) = \int p(\tilde{x}|\theta)p(\theta|X,\alpha)\,\text{d}\theta.
\end{equation}
In summary, Bayesian statistics provides us with a distribution of plausible parameter values from the \textit{posterior} and a distribution of hypothetical predicted values from the \textit{posterior predictive distribution}. We can then ask bespoke question of these probability distributions; for example, $P(\theta > 2|D, \alpha) = \int_{2}^{\infty}p(\theta|D\,\alpha)\,\text{d}\theta$ is the probability that a parameter is greater than $2$. For proteomics, these could be the probability that a fold-change exceeded a certain value or a the probability that a spectra belongs to a particular peptide. One possible interpretation of the prior is as a penalty that regularises the parameters, many applications exploit this and so can apply bespoke regularisation to their model. 

\subsection{Bayesian contributions to proteomics}
\subsubsection{Bottom-up proteomics and differential abundance}
Here, we highlight Bayesian approaches already applied to proteomics. We focus on proteomics data generated via mass-spectrometry but refer to contributions for the reverse-phase-protein-array (RPPA) literature \citep{Crook::2019, Ni::2019, Maity::2020} and 2D gel electrophoresis \citep{Morris::2011}. A number of approaches have been aimed at quantification and differential abundance analysis of bottom-up proteomics data \citep{Phillips::2021, The::2021, The::2019, Santra::2016, Peshkin::2019, Millikin::2020, Serang::2013, OBrien::2018, Serang::2012, Carvalho::2011}. \citet{Carvalho::2011} appreciate that Bayesian statistics can be used to improve analysis of spectral counting data using a Poisson likelihood and calculating the probability of detecting a protein in a particular sample. However, they do not exploit the full Bayesian toolkit by working with probability distributions and simply end-up interpreting their probabilities as $p$-values. Thus, it is not clear whether their approach is any better than simply using a likelihood-based approach. A number of approaches \citep{The::2021, The::2019, Peshkin::2019, Serang::2012} argue for propagating and quantifying the uncertainty in the analysis rather than simply the underlying quantitation values, either through relaxing the parsimony assumption \citep{Serang::2012}, including ion statistics via a two-level Beta-Binomial model \citep{Peshkin::2019}, or jointly modelling identification and quantitation statistics \citep{The::2019, The::2021}. \citet{Millikin::2020} argue to use an analogue of the t-test and exploit the posterior distribution by using an interval thresholding approach. \citet{OBrien::2018} note the inherent compositional nature of labelled proteomics approaches and include a modelling parameter to model ratio compression allowing better estimation of the true fold changes. Importantly this parameter is shared across all proteins, which allowed them to estimate the parameter accurately even for proteins with few observed peptides. \citet{Jow::2014} also model isobaric laballed mass-spectrometry data but do not model ratio compression. Meanwhile for label-free experiments, \citet{OBrien::2018b} explicitly model missingness showing that jointly modelling missingness and abundance leads to improved performance. All of these approaches demonstrate some benefit over previously applied approaches suggests that combining these method would provide further improvements. It also suggests translating these methods to other proteomics techniques would be a fruitful endeavour. However, many of the methods differ in a number of key modelling choices and it is not always clear how these choices were made.
\subsubsection{Protein and peptide identification}
One of the fundamental problems in mass-spectrometry based proteomics is identifying a peptide from a spectra. A spectra can be very noisy and $b-,y-$ ions can be missing which results in a complex observation process. Furthermore, we have prior knowledge of observing particular amino acid sequences and knowledge of the cleavage process. This appears an ideal scenario for application of Bayesian methods. Indeed, a number of approaches have been applied \citep{Chen::2005, Halloran::2016, Lewis::2018, Claassen::2009}. \citet{Chen::2005} use a fairly simple framework to calculate peptide identification probabilities based on peptide coordinates. \citet{Halloran::2016} employ a dynamic Bayesian network in a method called DRIP, allowing for insertions and deletions to the spectra. By modelling possible alignments between theoretical and observed spectra they can calculate the most probable peptide match. The authors find their approach improves over available methods particularly for low resolution MS2 data. \citet{Lewis::2018} take a different approach to the same problem, incorporating a scoring function into a likelihood model. Their model also allows deletions directly via indicator functions. Insertions are characterised by excessive deviations from the spectra of the candidate peptide, where excessive is characterised probabilistically via laplace noise. The authors also include prior information about possible cleavage pairs, as well as prior information about the probability of observing a particular peptide sequence in the dataset. Finally, in constrast to \citet{Halloran::2016} they make full use of Bayesian methods and provide a posterior distribution over possible peptides and parameters. This could allow multiple peptides to be associated to a spectrum with differing certainty which could be used in downstream analysis. \citet{Claassen::2009} tackle a slightly different problem and use a non-parametric Bayesian model to predict the coverage in sequential LC-MS/MS experiments but suggest their approach could also be adapted to database searching and de novo sequencing. Again, these approaches all have benefits over previously applied methodology. The clearest is the inclusion of more information and the ability to provide a flexible, and well rationalised, model to the underlying data. The ability to exploit uncertainty captured by the posterior distribution for downstream analysis is far more insightful that simply point estimates from a Bayesian analysis. However, these approaches have not been adopted by the community. This could be because the methods are difficult to apply, the benefits are not compelling or computation is excessive.
\subsubsection{Proteoforms and post-translation modifications}
A number of approaches are interested in applications to proteoform analysis (splice isoforms) or post-translational modifications \citep{Chung::2013, Webb::2014, Lim::2017, Shteynberg::2019, Mallikarun::2020}. \citet{Chung::2013} employ a non-parametric mixture model to jointly model the modification mass for each PTM group and the true (unobserved) location of the modified amino acid. Their approach outperforms other approaches, is fully automated and provides modification confidence scores. However, the approach does not model the underlying spectrum, which could result in unnecessary false positives. \citet{Webb::2014} tackle the proteoform problem by deconvolving peptides into signatures even if they are associated with the same protein. However, they only use a Bayesian point estimate rather than exploiting the full posterior distribution. \citet{Lim::2017} use a Bayesian model to estimate the phosphorylation stoichiometry using Bayesian statistics. By incorporating a physically plausible model they remove problems with previous models that could allow negative stoichiometry. Their joint model allows them to borrow power across replicates and they report downstream uncertainty. \citet{Shteynberg::2019} use a Bayesian mixture model to compute compute probabilities for modification sites. This allows them to combine precomputed scores in a rational way but again they do not examine the full posterior distributions. \citet{Mallikarun::2020} employ a Bayesian linear regression modelling strategy to analyse differential PTM data, suggesting their approach outperformed other methods and could allow uncertainty in missing values. The main benefit here appears to be the regularisation of the parameters using priors rather than specifically the uncertainty quantification in the analysis.

\subsubsection{Biomarkers and clinical proteomics}

Protein biomarkers, molecular indicators of aberrant processes or disease, and clinical proteomics are a key component of proteomics research. \citet{Hernandez::2015} provide a review of Bayesian method development in biomarker development and we refer to them for additional details. \citet{Morris::2006, Morris::2008} develop Bayesian wavelet-based functional mixed models for mass-spectrometry-based proteomics data. Their advanced framework, allows the simultaneous use of nonparametric fixed and random effects, which facilitates adjustment for clinical and experimental covariates that could affect the intensity and location of a spectra. Working with posterior distributions they are able to compute important quantities such as the probability of intensity changes for fixed fold levels and are able to control a Bayesian false discovery rate. \citet{Liao::2014} combine the above framework with image analysis methods to enable biomarker discovery from LC-MS data. \citet{Hwang::2008} develop a pipeline, MS-BID, for biomarker analysis which uses a Bayesian anova. \citet{Harris::2009} apply a Bayesian hierarchical linear probit regression model to determine discriminative biomarkers from mass spectrometry data. They find their approach improves over a K-nearest neighbour method. Furthermore, by using posterior probabilities they are able to determine which samples will the most promising for prognostics. \citet{Kuschner::2010} propose a Bayesian network to perform feature selection from mass-spectrometry data. The selected features then provided excellent predictive power. Though again this approach still uses a Bayesian point estimate and could have obtained more information by computing full posterior distributions. \citet{Deng::2007} develop a Bayesian network which allows them to integrate mass-spectrometry and microarray data, allowing them to borrow power between mRNA and protein levels. Here Bayesian statistics is sufficiently flexible to incorporate different modalities and weigh up the uncertainty between different datasets. More recently \citet{Liu::2020} developed Bayesian Function-on-Scalar quantile regression for mass-spectrometry data. This approach notes that biomarker difference may not be apparent at mean regression but rather at a particular quantile (such as the 0.95 quantile). This method simultaneously accounts for the functional nature of MALDI-TOF data, incorporates prior knowledge for adaptive regularization and a basis representation which allows borrowing of power. They find their method identifies biomarkers overlooked by mean regression. Bayesian methods for biomarker and clinical proteomics are more developed than other examined proteomics sub-fields with several exemplary methods that make full use of the flexibility of Bayesian modelling and the rich output of the posterior distribution. 
\subsubsection{Chromatography}
To facilitate identification in mass-spectrometry a liquid-chromatography step is usually applied. The time at which a peptide elutes from the liquid chromatography, called the retention time, can be used as additional information to help identify peptides. However, there is uncertainty in this retention time and they can vary from one run to another. \citet{Chen::2019} develop a Bayesian model called DART-ID, which models a latent (unobserved) global retention time alignment. This alignment allows them to combine the outputted posterior error probability of MaxQuant with the inferred RT density in each experiment. Hence, by using this result they can update their confidences and improve coverage in experiments by 50$\%$. Whilst their approach appears powerful, they only use a point estimate and obtain uncertainty through bootstrapping. Though the author reference computational challenges it would have been useful to examine the posterior distribution so we could be explicit about the computational trade-offs. \citet{Maboudi::2017} are interested in the uncertainty in peptide retention time methods. Using a Gaussian process regression method they were able to accurately predict retention times and obtain uncertainty estimates. They then use the posterior distribution from the regression analysis as a variable retention time window to identify potentially incorrect peptides. This improves over fixed windowing strategies. One strategy they overlooked, in a similar vain to DART-ID, would be to update the identification probabilities based the deviation probability from the predicted retention time. This approach naturally fits within a Bayesian framework. 

\subsubsection{Intact, top-down and structural proteomics}
Proteoform analysis is one of the key challenges in top-down proteomics. \citet{Leduc::2014} introduce a C-score, not be confused with a C-statistic, to facilitate automated identification and characeterisation of proteoforms from top-down proteomics data. Ultimately, their approach allows them to rank probable proteoforms having observed their data. Performing an analysis in a Bayesian framework allows them to specify a generative model, provide expert prior information and carefully model the underlying noise distribution. Their proposed C-score is essentially a transformed posterior error probability. However, despite their Bayesian framework, they opt for a point estimate of their model, which could have been greatly enhanced by examining the full posterior of their model. \citet{Marty::2015} proposed a Bayesian deconvolution algorithm for Ion Mobility spectra, and extended in \citet{Kostelic::2021}. Their approach allows the convolution of the charge distribution with the peak shape to obtain a flexible deconvolution approach. The extent of their applications is extensive, demonstrating a clear benefit of their method. However, their approach also uses a point estimate from their analysis. Indeed, the idea of a posterior distribution is not mentioned at all within the paper. Hence, apart from the use of prior information, it is not clear what particular benefit a Bayesian analysis has for their approach. \citet{Saltzberg::2017} propose a Bayesian model to resolve residue level information from hydrogen-deuterium exchange mass-spectrometry. They choose uninformative priors, and though they perform inference using Monte-Carlo methods they do not use the posterior distribution. Furthermore, they do not justify why there model allows for negative deuterium incorporation, which maybe arises from a misunderstanding of the positivity constraint induced by their exponential likelihood model.  

\subsubsection{Functional proteomics}       
Functional proteomics methods aim to decipher protein-function on a system-wide scale. One approach is spatial subcellular proteomics \citep{Geladaki::2019, Christopher::2021} where protein are localised to their subcellular niche using mass-spectrometry data. Bayesian approaches have been developed for biochemical fractionation-based subcellular proteomics \citep{Crook::2018, Crook::2019, Crook::2019b, Crook::2020, Crook::2021}. \citet{Crook::2018, Crook::2019, Crook::2019b} demonstrate Bayesian modelling can quantify uncertainty in protein subcellular localisation and identify cases where this may correspond to multi-localising proteins. \citet{Crook::2018} show that a even a Bayesian point estimate may overlook these cases and more information is obtained by examining the full posterior distribution. \citet{Crook::2020} allow the uncertainty in the number of subcellular niches to be accounted for and show that allowing additional niches can be uncovered. However, the model appears sensitive the prior choices and should be chosen carefully. \citet{Crook::2021} build on these experiments to analyse differential localisation experiments showing that modelling uncertainty improves power and interpretation compared with other methods. This fully Bayesian analysis; however, is computationally intensive as it attempts to model many datasets at once. Another functional approach is AP-MS, which allows us to determine protein interactions and complexes \citep{Christopher::2021}. \citet{choi::2010} develop a non-parametric Bayesian model to bi-cluster AP-MS data. They sample from the posterior distribution and are hence able to report the uncertainty in the clustering. However, their nested model assumes that the conditional on the Bait cluster the Prey clusters are independent and their model assumes exchangability (permutation leads to the same probability distribution) of the rows and columns. \citet{Fang::2021} propose a semi-parametric model for thermal protein profiling after identifying proteins that deviate from classic sigmoid behaviour. Semi-parametric models combine interpretable parametric models with more flexible non-parametric models. Using Bayesian analysis they can critically assess the semi-parametric and parametric model fits and demonstrate those that are better modelled by the semi-parametric model share functional enrichments. Again this fully Bayesian approach has demanding computational requirements. 
	
\subsection{The Bayesian workflow}
\subsubsection{Generative modelling}
Having highlighted the successes and limitations of some of the contributions of Bayesian methods to mass spectrometry-based proteomics, we clarify the Bayesian workflow to facilitate it for proteomics. The first tension of Bayesian analysis is the pairing of the likelihood and the prior. On one hand the word \textit{prior} suggests it must be chosen first; however, without knowledge of the likelihood it makes little sense to start selecting priors - we  may not even know the parameters of the model. Thinking of the likelihood and prior as a pair reduces this conceptual tension. It also leads to an explicit way to check our modelling assumptions via generative and predictive modelling. A generative model generates data consistent with the data. The prior has good predictive properties if the \textit{posterior predictive distribution} can predict new data generated from similar experiments. To be explicit, given a likelihood and prior, we can simulate data $y$. First, sample the parameters of the likelihood from the prior and then  given these parameters sample data from the model:	
\begin{equation}
\begin{split}
\tilde{\theta} &\sim p(\theta|\alpha) \\
\tilde{y} & \sim p(y|\tilde{\theta}).
\end{split}
\end{equation}
This leads us to define the \textit{prior predictive distribution}:
\begin{equation}
p(\tilde{y}|\alpha) = \int_{\theta} p(\tilde{y}|\theta)p(\theta|\alpha)\, \text{d}\theta.
\end{equation}
There are a number of key observations. Firstly, the prior predictive distribution has no knowledge of the data, aside from the modelling assumptions of the domain expert. Secondly, the likelihood and prior are now explicitly coupled and so poor modelling choices in either the likelihood or prior will be apparent via the prior predictive. Thirdly, the failure of uniform or uninformative priors as a default is clear, they will generate unrealistic data.

insert oops example

\subsubsection{Predictive modelling}
Once our prior and likelihood have seen the data, $D$, they are updated into the posterior distribution. We can then sample new data by first sampling parameters from the posterior distribution and then again sampling from the likelihood:
\begin{equation}
	\begin{split}
		\tilde{\theta} &\sim p(\theta|D, \alpha) \\
		\tilde{y} & \sim p(y|\tilde{\theta}).
	\end{split}
\end{equation}  
This leads to the definition of the posterior predictive distribution:
\begin{equation}
	p(\tilde{y}|D, \alpha) = \int_{\theta} p(\tilde{y}|\theta)p(\theta|D, \alpha)\, \text{d}\theta = \int_{\theta}p(\tilde{y}|\theta) \frac{p(D|\theta, \alpha)p(\theta|\alpha)}{p(D|\alpha)} \text{d}\theta.
\end{equation}
We have expanded integrand using Bayes theorem to make a key point explicit: the posterior predictive distribution depends on the likelihood, the prior and the data. This coupling allows us to make a number of observations. A good choice of prior and likelihood leads to good predictive performance and over-fitting can be examined via the posterior predictive distribution. 

insert oops example

\subsubsection{Fitting a model: Bayesian computation}
In practice, the integrals and probability distribution required for sufficiently flexible modelling are intractable. We can perform inference in a wide array of models using Markov-chain Monte Carlo (MCMC) methods, including Gibbs sampling, Metropolis sampling, and Hamiltonian Monte Carlo. Bayesian inference an also be performed using sequential Monte Carlo or variational inference. Although the latter can provide a fast approximation of the posterior distribution, it can be arbitrarily inaccurate. Here, we focus on Hamiltonian Monte Carlo, as it forms the basis of modern probabilistic programming languages. Initially, when an MCMC algorithm begins it will "move" towards the posterior distribution producing a "sample" at each iteration. An initial warm-up or burn-in section is require to remove bias due to dependence of the algorithms starting values and to adapt some of the algorithms tuning parameters to provide efficient inference. Once the warm-up section is complete, there is a sampling period which is run until multiple chains have mixed. One measure of mixing chains is $\hat{R}$, which is essentially a measure of between and within chain variance. We refer to for a precise mathematical description. 



	
\bibliographystyle{natbib}
\bibliography{BayesProtReviewBib}	
\end{document}